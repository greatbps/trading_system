#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
trading_system/backtesting/strategy_validator.py

ì „ëµ ê²€ì¦ê¸° - AI vs ë¹„-AI ì „ëµ ì„±ëŠ¥ ë¹„êµ ë° ê²€ì¦
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
import pandas as pd
import numpy as np
from scipy import stats

from .backtesting_engine import BacktestingEngine, BacktestResult, PerformanceMetrics

logger = logging.getLogger(__name__)

class ValidationStatus(Enum):
    """ê²€ì¦ ìƒíƒœ"""
    PASSED = "PASSED"
    FAILED = "FAILED"
    WARNING = "WARNING"
    INSUFFICIENT_DATA = "INSUFFICIENT_DATA"

@dataclass
class ValidationCriteria:
    """ê²€ì¦ ê¸°ì¤€"""
    min_return: float = 5.0  # ìµœì†Œ ì—°ìˆ˜ìµë¥  (%)
    max_drawdown: float = 20.0  # ìµœëŒ€ ë‚™í­ (%)
    min_sharpe: float = 1.0  # ìµœì†Œ ìƒ¤í”„ ë¹„ìœ¨
    min_win_rate: float = 45.0  # ìµœì†Œ ìŠ¹ë¥  (%)
    min_trades: int = 50  # ìµœì†Œ ê±°ë˜ ìˆ˜
    min_profit_factor: float = 1.2  # ìµœì†Œ ìˆ˜ìµ íŒ©í„°
    
    # AI ê´€ë ¨ ê¸°ì¤€
    min_ai_accuracy: float = 60.0  # ìµœì†Œ AI ì˜ˆì¸¡ ì •í™•ë„ (%)
    min_ai_improvement: float = 2.0  # AI ì‚¬ìš©ì‹œ ìµœì†Œ ì„±ëŠ¥ ê°œì„  (%)

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    strategy_name: str
    status: ValidationStatus
    criteria: ValidationCriteria
    
    # ì„±ëŠ¥ ê²€ì¦ ê²°ê³¼
    return_check: bool = False
    drawdown_check: bool = False
    sharpe_check: bool = False
    win_rate_check: bool = False
    trades_check: bool = False
    profit_factor_check: bool = False
    
    # AI ê²€ì¦ ê²°ê³¼
    ai_accuracy_check: bool = False
    ai_improvement_check: bool = False
    
    # ìƒì„¸ ë©”ì‹œì§€
    messages: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    @property
    def overall_score(self) -> float:
        """ì „ì²´ ê²€ì¦ ì ìˆ˜ (0-100)"""
        checks = [
            self.return_check, self.drawdown_check, self.sharpe_check,
            self.win_rate_check, self.trades_check, self.profit_factor_check,
            self.ai_accuracy_check, self.ai_improvement_check
        ]
        return (sum(checks) / len(checks)) * 100

@dataclass
class StrategyComparison:
    """ì „ëµ ë¹„êµ ê²°ê³¼"""
    strategy_name: str
    with_ai_result: BacktestResult
    without_ai_result: BacktestResult
    
    # ì„±ëŠ¥ ê°œì„  ì§€í‘œ
    return_improvement: float = 0.0
    sharpe_improvement: float = 0.0
    drawdown_improvement: float = 0.0
    win_rate_improvement: float = 0.0
    
    # í†µê³„ì  ìœ ì˜ì„±
    statistical_significance: bool = False
    p_value: float = 1.0
    
    # AI íš¨ê³¼ì„±
    ai_effectiveness_score: float = 0.0

class StrategyValidator:
    """ì „ëµ ê²€ì¦ê¸° - AI ê¸°ë°˜ ì „ëµ ì„±ëŠ¥ ê²€ì¦"""
    
    def __init__(self, config=None):
        """ì „ëµ ê²€ì¦ê¸° ì´ˆê¸°í™”"""
        try:
            if config:
                self.backtesting_engine = BacktestingEngine(config)
            else:
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”
                from config import get_config
                default_config = get_config()
                self.backtesting_engine = BacktestingEngine(default_config)
            self.logger = logging.getLogger(__name__)
        except Exception as e:
            self.logger = logging.getLogger(__name__)
            self.logger.warning(f"ë°±í…ŒìŠ¤íŒ… ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.backtesting_engine = None
    
    async def validate_strategy(
        self,
        strategy_name: str,
        backtest_result: BacktestResult,
        criteria: Optional[ValidationCriteria] = None
    ) -> ValidationResult:
        """
        ì „ëµ ê²€ì¦ ìˆ˜í–‰
        
        Args:
            strategy_name: ì „ëµ ì´ë¦„
            backtest_result: ë°±í…ŒìŠ¤íŒ… ê²°ê³¼
            criteria: ê²€ì¦ ê¸°ì¤€ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        
        Returns:
            ValidationResult: ê²€ì¦ ê²°ê³¼
        """
        try:
            if criteria is None:
                criteria = ValidationCriteria()
            
            result = ValidationResult(
                strategy_name=strategy_name,
                status=ValidationStatus.PASSED,
                criteria=criteria
            )
            
            metrics = backtest_result.metrics
            
            # 1. ìˆ˜ìµë¥  ê²€ì¦
            if metrics.annual_return >= criteria.min_return:
                result.return_check = True
                result.messages.append(f"âœ… ì—°ìˆ˜ìµë¥ : {metrics.annual_return:.2f}% (ê¸°ì¤€: {criteria.min_return}%)")
            else:
                result.return_check = False
                result.messages.append(f"âŒ ì—°ìˆ˜ìµë¥ : {metrics.annual_return:.2f}% (ê¸°ì¤€: {criteria.min_return}%)")
            
            # 2. ìµœëŒ€ ë‚™í­ ê²€ì¦
            if metrics.max_drawdown <= criteria.max_drawdown:
                result.drawdown_check = True
                result.messages.append(f"âœ… ìµœëŒ€ ë‚™í­: {metrics.max_drawdown:.2f}% (ê¸°ì¤€: {criteria.max_drawdown}%)")
            else:
                result.drawdown_check = False
                result.messages.append(f"âŒ ìµœëŒ€ ë‚™í­: {metrics.max_drawdown:.2f}% (ê¸°ì¤€: {criteria.max_drawdown}%)")
            
            # 3. ìƒ¤í”„ ë¹„ìœ¨ ê²€ì¦
            if metrics.sharpe_ratio >= criteria.min_sharpe:
                result.sharpe_check = True
                result.messages.append(f"âœ… ìƒ¤í”„ ë¹„ìœ¨: {metrics.sharpe_ratio:.2f} (ê¸°ì¤€: {criteria.min_sharpe})")
            else:
                result.sharpe_check = False
                result.messages.append(f"âŒ ìƒ¤í”„ ë¹„ìœ¨: {metrics.sharpe_ratio:.2f} (ê¸°ì¤€: {criteria.min_sharpe})")
            
            # 4. ìŠ¹ë¥  ê²€ì¦
            if metrics.win_rate >= criteria.min_win_rate:
                result.win_rate_check = True
                result.messages.append(f"âœ… ìŠ¹ë¥ : {metrics.win_rate:.2f}% (ê¸°ì¤€: {criteria.min_win_rate}%)")
            else:
                result.win_rate_check = False
                result.messages.append(f"âŒ ìŠ¹ë¥ : {metrics.win_rate:.2f}% (ê¸°ì¤€: {criteria.min_win_rate}%)")
            
            # 5. ê±°ë˜ ìˆ˜ ê²€ì¦
            if metrics.total_trades >= criteria.min_trades:
                result.trades_check = True
                result.messages.append(f"âœ… ì´ ê±°ë˜ ìˆ˜: {metrics.total_trades} (ê¸°ì¤€: {criteria.min_trades})")
            else:
                result.trades_check = False
                result.messages.append(f"âŒ ì´ ê±°ë˜ ìˆ˜: {metrics.total_trades} (ê¸°ì¤€: {criteria.min_trades})")
                if metrics.total_trades < 10:
                    result.status = ValidationStatus.INSUFFICIENT_DATA
            
            # 6. ìˆ˜ìµ íŒ©í„° ê²€ì¦
            if metrics.profit_factor >= criteria.min_profit_factor:
                result.profit_factor_check = True
                result.messages.append(f"âœ… ìˆ˜ìµ íŒ©í„°: {metrics.profit_factor:.2f} (ê¸°ì¤€: {criteria.min_profit_factor})")
            else:
                result.profit_factor_check = False
                result.messages.append(f"âŒ ìˆ˜ìµ íŒ©í„°: {metrics.profit_factor:.2f} (ê¸°ì¤€: {criteria.min_profit_factor})")
            
            # 7. AI ì •í™•ë„ ê²€ì¦ (AI ì‚¬ìš©ì‹œì—ë§Œ)
            if metrics.ai_accuracy > 0:
                if metrics.ai_accuracy >= criteria.min_ai_accuracy:
                    result.ai_accuracy_check = True
                    result.messages.append(f"âœ… AI ì˜ˆì¸¡ ì •í™•ë„: {metrics.ai_accuracy:.2f}% (ê¸°ì¤€: {criteria.min_ai_accuracy}%)")
                else:
                    result.ai_accuracy_check = False
                    result.messages.append(f"âŒ AI ì˜ˆì¸¡ ì •í™•ë„: {metrics.ai_accuracy:.2f}% (ê¸°ì¤€: {criteria.min_ai_accuracy}%)")
            else:
                result.ai_accuracy_check = True  # AI ë¯¸ì‚¬ìš©ì‹œ í†µê³¼
                result.messages.append("â„¹ï¸ AI ì˜ˆì¸¡ ì •í™•ë„: ë¯¸ì‚¬ìš©")
            
            # ì „ì²´ ê²€ì¦ ìƒíƒœ ê²°ì •
            basic_checks = [
                result.return_check, result.drawdown_check, result.sharpe_check,
                result.win_rate_check, result.trades_check, result.profit_factor_check
            ]
            
            if result.status != ValidationStatus.INSUFFICIENT_DATA:
                if all(basic_checks):
                    if result.ai_accuracy_check:
                        result.status = ValidationStatus.PASSED
                    else:
                        result.status = ValidationStatus.WARNING
                        result.warnings.append("AI ì˜ˆì¸¡ ì •í™•ë„ê°€ ê¸°ì¤€ì— ë¯¸ë‹¬")
                elif sum(basic_checks) >= 4:  # 6ê°œ ì¤‘ 4ê°œ ì´ìƒ í†µê³¼
                    result.status = ValidationStatus.WARNING
                    result.warnings.append("ì¼ë¶€ ê²€ì¦ ê¸°ì¤€ ë¯¸ë‹¬")
                else:
                    result.status = ValidationStatus.FAILED
            
            return result
            
        except Exception as e:
            self.logger.error(f"ì „ëµ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return ValidationResult(
                strategy_name=strategy_name,
                status=ValidationStatus.FAILED,
                criteria=criteria or ValidationCriteria(),
                messages=[f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"]
            )
    
    async def compare_ai_vs_traditional(
        self,
        strategy_name: str,
        start_date: datetime,
        end_date: datetime,
        symbols: List[str],
        initial_capital: float = 1000000.0
    ) -> StrategyComparison:
        """
        AI vs ì „í†µì  ì „ëµ ë¹„êµ
        
        Args:
            strategy_name: ì „ëµ ì´ë¦„
            start_date: ì‹œì‘ì¼
            end_date: ì¢…ë£Œì¼
            symbols: ëŒ€ìƒ ì¢…ëª©
            initial_capital: ì´ˆê¸° ìë³¸
        
        Returns:
            StrategyComparison: ë¹„êµ ê²°ê³¼
        """
        try:
            self.logger.info(f"AI vs ì „í†µì  ì „ëµ ë¹„êµ: {strategy_name}")
            
            # AI ì‚¬ìš© ë°±í…ŒìŠ¤íŒ…
            with_ai_result = await self.backtesting_engine.run_backtest(
                strategy_name, start_date, end_date, symbols,
                initial_capital, use_ai=True
            )
            
            # AI ë¯¸ì‚¬ìš© ë°±í…ŒìŠ¤íŒ…
            without_ai_result = await self.backtesting_engine.run_backtest(
                strategy_name, start_date, end_date, symbols,
                initial_capital, use_ai=False
            )
            
            # ë¹„êµ ê²°ê³¼ ìƒì„±
            comparison = StrategyComparison(
                strategy_name=strategy_name,
                with_ai_result=with_ai_result,
                without_ai_result=without_ai_result
            )
            
            # ì„±ëŠ¥ ê°œì„  ì§€í‘œ ê³„ì‚°
            ai_metrics = with_ai_result.metrics
            traditional_metrics = without_ai_result.metrics
            
            comparison.return_improvement = ai_metrics.annual_return - traditional_metrics.annual_return
            comparison.sharpe_improvement = ai_metrics.sharpe_ratio - traditional_metrics.sharpe_ratio
            comparison.drawdown_improvement = traditional_metrics.max_drawdown - ai_metrics.max_drawdown  # ë‚™í­ ê°ì†Œê°€ ê°œì„ 
            comparison.win_rate_improvement = ai_metrics.win_rate - traditional_metrics.win_rate
            
            # í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
            comparison.statistical_significance, comparison.p_value = await self._test_statistical_significance(
                with_ai_result, without_ai_result
            )
            
            # AI íš¨ê³¼ì„± ì ìˆ˜ ê³„ì‚° (0-100)
            comparison.ai_effectiveness_score = await self._calculate_ai_effectiveness(
                comparison
            )
            
            return comparison
            
        except Exception as e:
            self.logger.error(f"AI vs ì „í†µì  ì „ëµ ë¹„êµ ì˜¤ë¥˜: {e}")
            raise
    
    async def _test_statistical_significance(
        self,
        with_ai_result: BacktestResult,
        without_ai_result: BacktestResult
    ) -> Tuple[bool, float]:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì¦"""
        try:
            # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
            ai_returns = self._calculate_daily_returns(with_ai_result.equity_curve)
            traditional_returns = self._calculate_daily_returns(without_ai_result.equity_curve)
            
            if len(ai_returns) < 30 or len(traditional_returns) < 30:
                return False, 1.0
            
            # t-ê²€ì • ìˆ˜í–‰
            t_stat, p_value = stats.ttest_ind(ai_returns, traditional_returns)
            
            # ìœ ì˜ìˆ˜ì¤€ 5%ì—ì„œ ê²€ì¦
            is_significant = p_value < 0.05 and np.mean(ai_returns) > np.mean(traditional_returns)
            
            return is_significant, p_value
            
        except Exception as e:
            self.logger.error(f"í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ ì˜¤ë¥˜: {e}")
            return False, 1.0
    
    def _calculate_daily_returns(self, equity_curve: List[Dict]) -> List[float]:
        """ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°"""
        try:
            returns = []
            
            for i in range(1, len(equity_curve)):
                prev_value = equity_curve[i-1]['portfolio_value']
                curr_value = equity_curve[i]['portfolio_value']
                
                if prev_value > 0:
                    daily_return = (curr_value - prev_value) / prev_value
                    returns.append(daily_return)
            
            return returns
            
        except Exception as e:
            self.logger.error(f"ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚° ì˜¤ë¥˜: {e}")
            return []
    
    async def _calculate_ai_effectiveness(self, comparison: StrategyComparison) -> float:
        """AI íš¨ê³¼ì„± ì ìˆ˜ ê³„ì‚° (0-100)"""
        try:
            score = 0.0
            weight_sum = 0.0
            
            # ìˆ˜ìµë¥  ê°œì„  (ê°€ì¤‘ì¹˜: 30%)
            if comparison.return_improvement > 0:
                return_score = min(comparison.return_improvement / 10.0, 1.0) * 100
                score += return_score * 0.3
            weight_sum += 0.3
            
            # ìƒ¤í”„ ë¹„ìœ¨ ê°œì„  (ê°€ì¤‘ì¹˜: 25%)
            if comparison.sharpe_improvement > 0:
                sharpe_score = min(comparison.sharpe_improvement / 0.5, 1.0) * 100
                score += sharpe_score * 0.25
            weight_sum += 0.25
            
            # ë‚™í­ ê°œì„  (ê°€ì¤‘ì¹˜: 20%)
            if comparison.drawdown_improvement > 0:
                drawdown_score = min(comparison.drawdown_improvement / 5.0, 1.0) * 100
                score += drawdown_score * 0.2
            weight_sum += 0.2
            
            # ìŠ¹ë¥  ê°œì„  (ê°€ì¤‘ì¹˜: 15%)
            if comparison.win_rate_improvement > 0:
                winrate_score = min(comparison.win_rate_improvement / 10.0, 1.0) * 100
                score += winrate_score * 0.15
            weight_sum += 0.15
            
            # í†µê³„ì  ìœ ì˜ì„± (ê°€ì¤‘ì¹˜: 10%)
            if comparison.statistical_significance:
                score += 100 * 0.1
            weight_sum += 0.1
            
            return score / weight_sum if weight_sum > 0 else 0.0
            
        except Exception as e:
            self.logger.error(f"AI íš¨ê³¼ì„± ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    async def validate_multiple_strategies(
        self,
        strategies: List[str],
        start_date: datetime,
        end_date: datetime,
        symbols: List[str],
        criteria: Optional[ValidationCriteria] = None
    ) -> Dict[str, ValidationResult]:
        """ì—¬ëŸ¬ ì „ëµ ì¼ê´„ ê²€ì¦"""
        try:
            results = {}
            
            for strategy_name in strategies:
                self.logger.info(f"ì „ëµ ê²€ì¦: {strategy_name}")
                
                # ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
                backtest_result = await self.backtesting_engine.run_backtest(
                    strategy_name, start_date, end_date, symbols
                )
                
                # ê²€ì¦ ìˆ˜í–‰
                validation_result = await self.validate_strategy(
                    strategy_name, backtest_result, criteria
                )
                
                results[strategy_name] = validation_result
            
            return results
            
        except Exception as e:
            self.logger.error(f"ë‹¤ì¤‘ ì „ëµ ê²€ì¦ ì˜¤ë¥˜: {e}")
            raise
    
    async def generate_validation_report(
        self,
        validation_results: Dict[str, ValidationResult],
        comparison_results: Optional[Dict[str, StrategyComparison]] = None
    ) -> str:
        """ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        try:
            report = []
            report.append("=" * 80)
            report.append("ğŸ” ì „ëµ ê²€ì¦ ë³´ê³ ì„œ")
            report.append("=" * 80)
            report.append(f"ğŸ“… ë³´ê³ ì„œ ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report.append("")
            
            # ê²€ì¦ ê²°ê³¼ ìš”ì•½
            passed_count = sum(1 for r in validation_results.values() if r.status == ValidationStatus.PASSED)
            warning_count = sum(1 for r in validation_results.values() if r.status == ValidationStatus.WARNING)
            failed_count = sum(1 for r in validation_results.values() if r.status == ValidationStatus.FAILED)
            
            report.append("ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½")
            report.append("-" * 40)
            report.append(f"âœ… í†µê³¼: {passed_count}ê°œ")
            report.append(f"âš ï¸ ê²½ê³ : {warning_count}ê°œ")
            report.append(f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
            report.append(f"ğŸ“ˆ ì´ ì „ëµ: {len(validation_results)}ê°œ")
            report.append("")
            
            # ê°œë³„ ì „ëµ ê²°ê³¼
            for strategy_name, result in validation_results.items():
                report.append(f"ğŸ¯ ì „ëµ: {strategy_name}")
                report.append(f"ğŸ“Š ìƒíƒœ: {result.status.value}")
                report.append(f"ğŸ”¢ ì „ì²´ ì ìˆ˜: {result.overall_score:.1f}ì ")
                report.append("")
                
                for msg in result.messages:
                    report.append(f"   {msg}")
                
                if result.warnings:
                    report.append("   âš ï¸ ê²½ê³ :")
                    for warning in result.warnings:
                        report.append(f"     - {warning}")
                
                report.append("")
            
            # AI vs ì „í†µì  ì „ëµ ë¹„êµ (ìˆëŠ” ê²½ìš°)
            if comparison_results:
                report.append("ğŸ¤– AI vs ì „í†µì  ì „ëµ ë¹„êµ")
                report.append("-" * 40)
                
                for strategy_name, comparison in comparison_results.items():
                    report.append(f"ğŸ“ˆ ì „ëµ: {strategy_name}")
                    report.append(f"ğŸ¯ AI íš¨ê³¼ì„± ì ìˆ˜: {comparison.ai_effectiveness_score:.1f}ì ")
                    report.append(f"ğŸ“Š ìˆ˜ìµë¥  ê°œì„ : {comparison.return_improvement:+.2f}%")
                    report.append(f"ğŸ“ˆ ìƒ¤í”„ ë¹„ìœ¨ ê°œì„ : {comparison.sharpe_improvement:+.2f}")
                    report.append(f"ğŸ“‰ ë‚™í­ ê°œì„ : {comparison.drawdown_improvement:+.2f}%")
                    report.append(f"ğŸ¯ ìŠ¹ë¥  ê°œì„ : {comparison.win_rate_improvement:+.2f}%")
                    
                    if comparison.statistical_significance:
                        report.append(f"âœ… í†µê³„ì  ìœ ì˜ì„±: ìœ ì˜í•¨ (p-value: {comparison.p_value:.4f})")
                    else:
                        report.append(f"âŒ í†µê³„ì  ìœ ì˜ì„±: ìœ ì˜í•˜ì§€ ì•ŠìŒ (p-value: {comparison.p_value:.4f})")
                    
                    report.append("")
            
            return "\n".join(report)
            
        except Exception as e:
            self.logger.error(f"ê²€ì¦ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"